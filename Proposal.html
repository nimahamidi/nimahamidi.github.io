<p style="text-align: right;"><strong>CS:4980, Deep Learning <br /></strong><strong><span style="color: #a5a5a5;">NIMA HAMIDI<br /></span></strong><em>Project Proposal </em></p>
<p style="text-align: center; border: none; padding: 0in;"><strong><span style="color: #ed7d31;">AUTOMATIC MUSIC STRUCTURE ANALYSIS USING DEEP NEURAL NETWORK</span></strong></p>
<p style="text-align: center;"><strong><span style="color: black;">BUILDING A MODEL THAT TAKES A RECORDED MUSICAL EXAMPLE AND EXTRACTS THE FORMAL STRUCTURE OF THE PIECE, UTILIZING NEURAL NETWORK.</span></strong></p>
<p style="text-align: justify;">Over the last couple of years, I have been working on an automatic tool to analyze music in a temporal form. In order to understand the concept, I will describe the traditional approaches to music analysis. &nbsp;</p>
<p><strong>Music Analysis:</strong></p>
<p style="text-align: justify;">There is a long tradition of analyzing music to understand the segmentation, considering the perception of time. A very simple form of A -B -A is a piece of music that is consist of three main statements or musical segments. The first statement &ldquo;A&rdquo; is followed by a contrasting idea &ldquo;B&rdquo;. The &ldquo;A&rdquo; section comes back at the end to conclude the musical expression. This way of analysis can go further and deconstruct a complex piece of music into several segments. The temporal relationship between segments defines the unique auditory experience.&nbsp;</p>
<p style="text-align: right;">&nbsp;</p>
<p style="text-align: center; tab-stops: 207.35pt;"><em>A piece of music in the form of A-B-A</em></p>
<p style="text-align: center; tab-stops: 207.35pt;"><em>&nbsp;</em></p>
<p style="text-align: justify; tab-stops: 207.35pt;">Traditionally there are a large number of high-level feature in order to come up with such a segmentation. Harmony, phrasing, voicing, instrumentation, dynamic, pitch structure, and rhythm are some of this musical feature. Extracting some of these features are very challenging and eventually not producing the best result.</p>
<p style="tab-stops: 207.35pt;"><strong>Visualizing temporality in Music:</strong></p>
<p style="text-align: justify; tab-stops: 207.35pt;">I propose that three mid-level spectral feature of loudness, rhythmic density (onset detection) and roughness (Timbre) are critical to shape temporal experience. In my recent research, I utilized two of these features to propose a visualization that conveys a temporal understanding of the intended piece. I compare the result of this automatic tool to a traditional analysis, done using music score and human aural skills (Black vertical lines).</p>
<p style="text-align: justify; tab-stops: 207.35pt;">Two features of loudness (heatmap) and density (blue scatter plot) are the mid-level features. Using some simple algorithms and statistical smoothing function I extracted red and blue segmentation that represent two different features individually. As you can see in Figure 3.21<a href="#_ftn1" name="_ftnref1"><span style="font-size: 12.0pt; font-family: 'Calibri',sans-serif;">[1]</span></a> The black vertical lines, resulting from a traditional manual analysis, are coinciding with the proposed segmentation by the algorithm most of the time. However, to convey a holistic temporal structure, it seems relevant to propose the segmentation, considering all the intended features combined.</p>
<p style="tab-stops: 207.35pt;"><strong>Project Proposal </strong></p>
<p style="text-align: justify; tab-stops: 0in;">I propose to build a model that takes a recorded musical example and extracts the formal structure of the piece utilizing Neural Network. The process is as the following.</p>
<ul>
<li style="text-indent: -.25in; tab-stops: 207.35pt;">Extract three mid-level feature from the audio file, utilizing signal processing approaches.</li>
<li style="text-indent: -.25in; tab-stops: 207.35pt;">Generate two-dimensional matrix that carries three set of features. This can be treated as a three channel image data which contains loudness, density, and roughness instead of RGB channels.</li>
<li style="text-indent: -.25in; tab-stops: 207.35pt;">Design and tune a neural network such as <em>Autoencoder</em> that is proper for unsupervised learning tasks.</li>
<li style="text-indent: -.25in; tab-stops: 207.35pt;">Extract the temporal segmentation using clustering techniques.</li>
</ul>
<p style="tab-stops: 0in;"><strong>Unsupervised Learning - Autoencoder: </strong></p>
<p style="text-align: justify; tab-stops: 0in;">An autoencoder is an artificial neural network used for unsupervised. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. The autoencoder idea is motivated by the concept of good representation, in other words it states that not all the representations or features of the input are suitable to perform the tasks such as classification. <a href="#_ftn2" name="_ftnref2"><span style="font-size: 12.0pt; font-family: 'Calibri',sans-serif;">[2]</span></a></p>
<p style="tab-stops: 207.35pt;"><strong>Software - Details</strong></p>
<p style="text-align: justify; tab-stops: 0in;">The first portion of the code will extract three features of loudness, density, and roughness in a form of three channel, two-dimensional array. For this portion, I will use signal processing libraries such as <em>Librosa, Essentia, and numpy.</em> The second portion is an autoencoder neural network that reconstructs the input array each iteration. The cost function is the difference between the original and the reconstructed piece. The network learns by reducing the cost function to the minimum. After several epochs, some of the hidden layer might include proper data in order to extract the temporal structure.&nbsp; For this portion, I will used <em>Tensorflow, Keras, </em>or the combination of these two.</p>
<p style="tab-stops: 207.35pt;"><strong>Papers Studied:</strong></p>
<p>Deep Learning for Sequential Pattern Recognition, Pooyan Safari</p>
<p style="tab-stops: 207.35pt;">Musical Structure Segmentation with Convolutional Neural Networks, Tim O&rsquo;Brien</p>
<p style="tab-stops: 207.35pt;">Composing Music with Recurrent Neural Networks, Daniel Johnson</p>
<p style="tab-stops: 207.35pt;"><strong><span style="color: black;">Future Work:</span></strong></p>
<p style="text-align: justify; tab-stops: 207.35pt;"><span style="color: black;">A successful result from this project will provide the proper knowledge and tool to design a more complex system, capable of reconstructing musical pieces. At this point, composing music utilizing neural networks, such as convolutional neural network and RNN, mimicking a specific style or composer is feasible. Since these models are looking at small musical elements such as rhythmic pattern, harmonic progression and melodic figures, the final result is missing a holistic expression. I proposed this issue can be addressed by using my proposed approach. Instead of looking at the small musical feature, considering a temporal understanding of a large number of musical examples, one might propose an optimal structure that fulfills holistic expression.&nbsp; In other word, this will provide a tool that takes a large number of musical examples, and learn the temporal pattern carried over all the examples. The machine can compose a new piece that reflects the temporal feature, learned from a collection of musical example. Indeed, considering style or a specific composer might be relevant. </span></p>
<p style="text-align: justify; tab-stops: 207.35pt;">&nbsp;</p>
<p><a href="#_ftnref1" name="_ftn1"><span style="font-size: 10.0pt; font-family: 'Calibri',sans-serif;">[1]</span></a> Visualizing Temporality &ndash; Nima Hamidi &ndash; PhD Dissertation in Music Composition</p>
<p><a href="#_ftnref2" name="_ftn2"><span style="font-size: 10.0pt; font-family: 'Calibri',sans-serif;">[2]</span></a> Deep Learning for Sequential Pattern Recognition, Pooyan Safari</p>